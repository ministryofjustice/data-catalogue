# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
datahub-gms:
  enabled: true
  image:
    repository: acryldata/datahub-gms
  extraEnvs:
    - name: "BUSINESS_ATTRIBUTE_ENTITY_ENABLED"
      value: "true"
  securityContext:
    runAsUser: 1000
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsNonRoot: true
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]
  resources:
    limits:
      memory: 2Gi
    requests:
      cpu: 1000m
      memory: 2Gi
  livenessProbe:
    initialDelaySeconds: 60
    periodSeconds: 30
    failureThreshold: 8
  readinessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  theme_v2:
    enabled: true
    default: false
    toggeable: true
  service:
    type: ClusterIP
  serviceMonitor:
    create: true

datahub-frontend:
  enabled: true
  replicaCount: 1
  hpa:
    # If enabled, Horizontal Pod Autoscaler (HPA) will automatically scale the number of replicas based on CPU and memory utilization thresholds.
    enabled: false
    minReplicas: 2
    maxReplicas: 3
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Pods
            value: 1
            periodSeconds: 180
      scaleUp:
        stabilizationWindowSeconds: 0
        policies:
          - type: Pods
            value: 1
            periodSeconds: 60
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 70
  fullnameOverride: ""
  image:
    repository: acryldata/datahub-frontend-react
    # tag: "v0.10.0" # # defaults to .global.datahub.version
  extraVolumes:
    - name: datahub-users
      secret:
        defaultMode: 0444
        secretName: datahub-users-secret
  extraVolumeMounts:
    - name: datahub-users
      mountPath: /datahub-frontend/conf/user.props
      subPath: user.props
  extraEnvs:
    - name: "AUTH_OIDC_EXTRACT_GROUPS_ENABLED"
      value: "true"
    # Allow both user/pass login & SSO authentication https://datahubproject.io/docs/authentication/guides/add-users/
    - name: "AUTH_JAAS_ENABLED"
      value: "true"
    - name: "OPENSEARCH_USE_AWS_IAM_AUTH"
      value: "true"
    - name: "BUSINESS_ATTRIBUTE_ENTITY_ENABLED"
      value: "true"
  service:
    type: ClusterIP
    extraLabels: {}
  serviceMonitor:
    create: true
  lifecycle: {}
  resources:
    limits:
      memory: 1400Mi
    requests:
      cpu: 100m
      memory: 512Mi
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsNonRoot: true
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]
  # Set up ingress to expose react front-end
  ingress:
    enabled: true
    className: modsec
    annotations:
      external-dns.alpha.kubernetes.io/set-identifier: ""
      external-dns.alpha.kubernetes.io/aws-weight: "100"
      nginx.ingress.kubernetes.io/limit-whitelist: ""
      nginx.ingress.kubernetes.io/limit-rps: "200"
      nginx.ingress.kubernetes.io/enable-modsecurity: "true"
      nginx.ingress.kubernetes.io/modsecurity-snippet: |
        SecRuleEngine On
        SecRequestBodyAccess On
        SecRule REQUEST_URI "@rx ^\/api\/graphql|\/api\/v2\/graphql|\/api\/gms\/api\/graphql$" "id:1001,phase:2,t:none,nolog,pass,ctl:ruleRemoveById=932100,ctl:ruleRemoveById=932105,ctl:ruleRemoveById=932115,ctl:ruleRemoveById=932150,ctl:ruleRemoveById=949110;ARGS:json.query"
        SecRule REQUEST_URI "@rx ^\/api\/gms\/entities\?action=ingest|\/api\/gms\/aspects\?action=ingestProposal|\/api\/gms\/aspects\?action=ingestProposalBatch$" "id:1002,phase:1,t:none,nolog,pass,ctl:ruleRemoveById=930120,ctl:ruleRemoveById=933210,ctl:ruleRemoveById=933160,ctl:ruleRemoveById=949110,ctl:ruleRemoveById=200002"
        SecRule REQUEST_URI "@contains .profile" "id:1003,phase:1,t:lowercase,nolog,pass,ctl:ruleRemoveById=930130,ctl:ruleRemoveById=930120"
        SecRule REQUEST_HEADERS:Content-Type "^application/json" "id:1004,phase:1,t:none,t:lowercase,pass,nolog,ctl:requestBodyProcessor=JSON"
        SecRule ARGS:json.urn "@contains .profile" "id:1005,phase:2,t:lowercase,nolog,pass,ctl:ruleRemoveById=930130,ctl:ruleRemoveById=930120"
        SecRule REQUEST_URI "@contains nomis_internet" "id:1006,phase:2,t:none,nolog,pass,ctl:ruleRemoveById=933150"
        SecRule REQUEST_URI "@contains performance_kpis_intermediate" "id:1007,phase:2,t:none,nolog,pass,ctl:ruleRemoveById=933150"
        SecRule ARGS:json.urn "@contains performance_kpis_intermediate" "id:1008,phase:2,t:lowercase,nolog,pass,ctl:ruleRemoveById=933150"
        SecRule ARGS:json.urn "@contains nomis_internet" "id:1009,phase:2,t:lowercase,nolog,pass,ctl:ruleRemoveById=933150"
        SecRule REQUEST_COOKIES:REDIRECT_URL "@contains mkdir" "id:1010,phase:2,t:lowercase,nolog,pass,ctl:ruleRemoveById=933150"
        SecDefaultAction "phase:2,pass,log,tag:github_team=data-catalogue"
        SecDefaultAction "phase:4,pass,log,tag:github_team=data-catalogue"
    tls:
      - hosts:
          - ""
    hosts:
      - host: ""
        paths:
          - /
  oidcAuthentication:
    enabled: true
    provider: azure
    clientId: ""
    clientSecretRef:
      secretRef: azure-secrets
      secretKey: client_secret
    azureTenantId: ""
    # user_name_claim: "email" # default "email"
    # user_name_claim_regex: "([^@]+)" # "([^@]+)"
  # opt-in to metadata service auth for frontend proxy service: https://github.com/acryldata/datahub/blob/master/docs/authentication/introducing-metadata-service-authentication.md#configuring-metadata-service-authentication
  auth:
    enabled: true

acryl-datahub-actions:
  enabled: true
  image:
    repository: acryldata/datahub-actions
    tag: "v0.2.0"
  serviceAccount:
    name: ""
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsNonRoot: true
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi
  serviceMonitor:
    create: true

datahub-ingestion-cron:
  enabled: false

elasticsearchSetupJob:
  enabled: true
  serviceAccount: ""
  image:
    repository: acryldata/datahub-elasticsearch-setup
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  extraEnvs:
    - name: USE_AWS_ELASTICSEARCH
      value: "true"
    - name: OPENSEARCH_USE_AWS_IAM_AUTH
      value: "true"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars: []
  securityContext:
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsNonRoot: true
    runAsUser: 1000
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]

kafkaSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-kafka-setup
    # tag: "v0.11.0" # defaults to .global.datahub.version
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 768Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsNonRoot: true
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  extraSidecars: []

mysqlSetupJob:
  enabled: false

postgresqlSetupJob:
  enabled: true
  image:
    repository: acryldata/datahub-postgres-setup
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 101
  securityContext:
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsNonRoot: true
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  extraSidecars: []
  extraEnvs:
    - name: "DATAHUB_DB_NAME"
      valueFrom:
        secretKeyRef:
          name: rds-postgresql-instance-output
          key: database_name

## No code data migration
datahubUpgrade:
  enabled: true
  image:
    repository: acryldata/datahub-upgrade
    # tag: "v0.11.0"  # defaults to .global.datahub.version
  batchSize: 1000
  batchDelayMs: 500
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsNonRoot: true
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]
  podAnnotations: {}
  extraSidecars: []
  restoreIndices:
    # See https://docs.datahub.com/docs/how/restore-indices/#kubernetes for more parameters
    args:
      batchSize: "1000"
      batchDelayMs: "500"
      # Uses a key based paging strategy when scrolling through SQL rows instead of offset. Often faster for large sets of data
      urnBasedPagination: "true"
      # Specifies number of threads for processing pages of rows to restore
      numThreads: "3"
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
    # Schedule of CronJob when enabled
    schedule: "30 01 * * 6" # Every Saturday at 01:30
    concurrencyPolicy: Allow
    extraSidecars: []
    extraEnvs:
      - name: "DATAHUB_DB_NAME"
        valueFrom:
          secretKeyRef:
            name: rds-postgresql-instance-output
            key: database_name
  extraInitContainers: []

## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
datahubSystemUpdate:
  image:
    repository: acryldata/datahub-upgrade
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    runAsUser: 1000
    runAsNonRoot: true
    capabilities:
      drop: ["ALL"]
      add: ["NET_BIND_SERVICE"]
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-4"
    helm.sh/hook-delete-policy: before-hook-creation
  nonblocking:
    enabled: true
    annotations:
      # This is what defines this resource as a hook. Without this line, the
      # job is considered part of the release.
      helm.sh/hook: post-install,post-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Depends on v0.14.2 or greater
  bootstrapMCPs:
    default:
      value_configs:
        - "datahub.bootstrapMCPs.default.ingestion.version"
        - "datahub.bootstrapMCPs.default.schedule.timezone"
  resources:
    limits:
      cpu: 800m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi
  extraSidecars: []
  extraInitContainers: []

global:
  strict_mode: true
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: false
  imageRegistry: "docker.io"

  elasticsearch:
    #  The host is the core of the proxy url (minus "https://" prefix and ":port" suffix)
    host: ""
    port: "8080"
    useSSL: "false"

    metadataChangeLog:
      hooks:
        siblings:
          enabled: true
          consumerGroupSuffix: ""
        updateIndices:
          enabled: true
          consumerGroupSuffix: ""
        ingestionScheduler:
          enabled: true
          consumerGroupSuffix: ""
        incidents:
          enabled: true
          consumerGroupSuffix: ""
        entityChangeEvents:
          enabled: true
          consumerGroupSuffix: ""
        forms:
          enabled: true
          consumerGroupSuffix: ""

    ## The following section controls when and how reindexing of elasticsearch indices are performed
    index:
      ## Enable reindexing when mappings change based on the data model annotations
      enableMappingsReindex: true

      ## Enable reindexing when static index settings change.
      ## Dynamic settings which do not require reindexing are not affected
      ## Primarily this should be enabled when re-sharding is necessary for scaling/performance.
      enableSettingsReindex: true

      ## Index settings can be overridden for entity indices or other indices on an index by index basis
      ## Some index settings, such as # of shards, requires reindexing while others, i.e. replicas, do not
      ## Non-Entity indices do not require the prefix
      # settingsOverrides: '{"graph_service_v1":{"number_of_shards":"5"},"system_metadata_service_v1":{"number_of_shards":"5"}}'
      ## Entity indices do not require the prefix or suffix
      # entitySettingsOverrides: '{"dataset":{"number_of_shards":"10"}}'

      ## The amount of delay between indexing a document and having it returned in queries
      ## Increasing this value can improve performance when ingesting large amounts of data
      # refreshIntervalSeconds: 1

      ## The following options control settings for datahub-upgrade job when creating or reindexing indices
      upgrade:
        ## When reindexing is required, this option will clone the existing index as a backup
        ## The clone indices are not currently managed.
        cloneIndices: true

        ## Typically when reindexing the document counts between the original and destination indices should match.
        ## In some cases reindexing might not be able to proceed due to incompatibilities between a document in the
        ## orignal index and the new index's mappings. This document could be dropped and re-ingested or restored from
        ## the SQL database.
        ##
        ## This setting allows continuing if and only if the cloneIndices setting is also enabled which
        ## ensures a complete backup of the original index is preserved.
        allowDocCountMismatch: false

    ## Search related configuration
    search:
      ## Maximum terms in aggregations
      maxTermBucketSize: 100

      ## Configuration around exact matching for search
      exactMatch:
        ## if false will only apply weights, if true will exclude non-exact
        exclusive: false
        ## include prefix exact matches
        withPrefix: true
        ## boost multiplier when exact with case
        exactFactor: 2.0
        ## boost multiplier when exact prefix
        prefixFactor: 1.6
        ## stacked boost multiplier when case mismatch
        caseSensitivityFactor: 0.7
        ## enable exact match on structured search
        enableStructured: true

      ## Configuration for graph service dao
      graph:
        ## graph dao timeout seconds
        timeoutSeconds: 50
        ## graph dao batch size
        batchSize: 1000
        ## graph dao max result size
        maxResult: 10000

      custom:
        enabled: true
        # See documentation: https://datahubproject.io/docs/how/search/#customizing-search
        # default config: https://github.com/acryldata/datahub-helm/blob/master/charts/datahub/values.yaml#L481-L554
        config:
          # Notes:
          #
          # First match wins
          #
          # queryRegex = Java regex syntax
          #
          # functionScores - See the following for function score syntax
          # https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-function-score-query.html

          queryConfigurations:
            # Select */explore all
            # Attempt to rank active incidents at the top followed by enrichment factors
            - queryRegex: "[*]|"
              simpleQuery: false
              prefixMatchQuery: false
              exactMatchQuery: false
              functionScore:
                functions:
                  - filter:
                      term:
                        hasActiveIncidents:
                          value: true
                    weight: 2.0
                  - filter:
                      term:
                        hasDescription:
                          value: true
                    weight: 1.25
                  - filter:
                      term:
                        hasOwners:
                          value: true
                    weight: 1.25
                  - filter:
                      term:
                        hasDomain:
                          value: true
                    weight: 1.1
                  - filter:
                      term:
                        hasGlossaryTerms:
                          value: true
                    weight: 1.1
                  - filter:
                      term:
                        hasTags:
                          value: true
                    weight: 1.1
                  - filter:
                      term:
                        hasRowCount:
                          value: true
                    weight: 1.05
                  - filter:
                      term:
                        hasColumnCount:
                          value: true
                    weight: 1.05
                  - filter:
                      term:
                        deprecated:
                          value: true
                    weight: 0.5
                score_mode: multiply
                boost_mode: multiply

            # Is a single term with `_`, `.`, or `-`
            - queryRegex: >-
                ^[a-zA-Z0-9]\S+[_.-]\S+[a-zA-Z0-9]$
              simpleQuery: false
              prefixMatchQuery: true
              exactMatchQuery: true
              functionScore:
                functions:
                  - filter:
                      term:
                        deprecated:
                          value: true
                    weight: 0.5
                score_mode: multiply
                boost_mode: multiply

            # Quoted search strings
            - queryRegex: >-
                ^["'].+["']$
              simpleQuery: false
              prefixMatchQuery: true
              exactMatchQuery: true
              functionScore:
                functions:
                  - filter:
                      term:
                        deprecated:
                          value: true
                    weight: 0.5
                score_mode: multiply
                boost_mode: multiply

            # default
            - queryRegex: .*
              simpleQuery: true
              prefixMatchQuery: true
              exactMatchQuery: true
              functionScore:
                functions:
                  - filter:
                      prefix:
                        urn:
                          value: "urn:li:container:"
                    weight: 1000.0
                score_mode: multiply
                boost_mode: multiply
  kafka:
    config:
      log.retention.hours: 24
    bootstrap:
      server: "prerequisites-kafka:9092"
    zookeeper:
      server: "prerequisites-zookeeper:2181"
    # This section defines the names for the kafka topics that DataHub depends on, at a global level. Do not override this config
    # at a sub-chart level.
    topics:
      metadata_change_event_name: "MetadataChangeEvent_v4"
      failed_metadata_change_event_name: "FailedMetadataChangeEvent_v4"
      metadata_audit_event_name: "MetadataAuditEvent_v4"
      datahub_usage_event_name: "DataHubUsageEvent_v1"
      metadata_change_proposal_topic_name: "MetadataChangeProposal_v1"
      failed_metadata_change_proposal_topic_name: "FailedMetadataChangeProposal_v1"
      metadata_change_log_versioned_topic_name: "MetadataChangeLog_Versioned_v1"
      metadata_change_log_timeseries_topic_name: "MetadataChangeLog_Timeseries_v1"
      platform_event_topic_name: "PlatformEvent_v1"
      datahub_upgrade_history_topic_name: "DataHubUpgradeHistory_v1"
    consumer_groups:
      datahub_upgrade_history_kafka_consumer_group_id: {}
      #   gms: "<<release-name>>-duhe-consumer-job-client-gms"
      #   mae-consumer: "<<release-name>>-duhe-consumer-job-client-mcl"
      #   mce-consumer: "<<release-name>>-duhe-consumer-job-client-mcp"
      datahub_actions_doc_propagation_consumer_group_id: "datahub_doc_propagation_action"
      datahub_actions_ingestion_executor_consumer_group_id: "ingestion_executor"
      datahub_actions_slack_consumer_group_id: "datahub_slack_action"
      datahub_actions_teams_consumer_group_id: "datahub_teams_action"
      datahub_usage_event_kafka_consumer_group_id: "datahub-usage-event-consumer-job-client"
      metadata_change_log_kafka_consumer_group_id: "generic-mae-consumer-job-client"
      platform_event_kafka_consumer_group_id: "generic-platform-event-job-client"
      metadata_change_event_kafka_consumer_group_id: "mce-consumer-job-client"
      metadata_change_proposal_kafka_consumer_group_id: "generic-mce-consumer-job-client"
    metadataChangeLog:
      hooks:
        siblings:
          enabled: true
          consumerGroupSuffix: ""
        updateIndices:
          enabled: true
          consumerGroupSuffix: ""
        ingestionScheduler:
          enabled: true
          consumerGroupSuffix: ""
        incidents:
          enabled: true
          consumerGroupSuffix: ""
        entityChangeEvents:
          enabled: true
          consumerGroupSuffix: ""
        forms:
          enabled: true
          consumerGroupSuffix: ""
    maxMessageBytes: "5242880" # 5MB
    producer:
      compressionType: none
      maxRequestSize: "5242880" # 5MB
    consumer:
      maxPartitionFetchBytes: "5242880" # 5MB
      stopContainerOnDeserializationError: true
    ## For AWS MSK set this to a number larger than 1
    # partitions: 3
    # replicationFactor: 3
    schemaregistry:
      # GMS Implementation - `url` configured based on component context
      type: INTERNAL
      # Glue Implementation - `url` not applicable
      # type: AWS_GLUE
      # glue:
      #   region: us-east-1
      #   registry: datahub

  sql:
    datasource:
      host: ""
      # as host, minus ":port"
      hostForpostgresqlClient: ""
      port: "5432"
      # url is format "jdbc:postgresql://{host}/{database_name}"
      url: ""
      driver: "org.postgresql.Driver"
      username: ""
      password:
        secretRef: rds-postgresql-instance-output
        secretKey: database_password
      extraEnvs:
        - name: "DATAHUB_DB_NAME"
          value: ""

  datahub:
    version: v1.1.0
    gms:
      port: "8080"
      nodePort: "30001"

    # Used for scheduled tasks
    timezone: "Europe/London"

    frontend:
      validateSignUpEmail: true

    monitoring:
      enablePrometheus: true

    mae_consumer:
      port: "9091"
      nodePort: "30002"

    appVersion: "1.0.0"
    systemUpdate:
      ## The following options control settings for datahub-upgrade job which will
      ## managed ES indices and other update related work
      enabled: true

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      provisionSecret:
        enabled: true
        autoGenerate: true
        annotations: {}

    managed_ingestion:
      enabled: true
      defaultCliVersion: "1.0.0"

    metadata_service_authentication:
      enabled: true
      systemClientId: "__datahub_system"
      systemClientSecret:
        secretRef: "datahub-auth-secrets"
        secretKey: "system_client_secret"
      tokenService:
        signingKey:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_signing_key"
        salt:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_salt"
      provisionSecrets:
        enabled: true
        autoGenerate: true
        annotations: {}

    ## Enables always emitting a MCL even when no changes are detected. Used for Time Based Lineage when no changes occur.
    alwaysEmitChangeLog: false

    ## Enables diff mode for graph writes, uses a different code path that produces a diff from previous to next to write relationships instead of wholesale deleting edges and reading
    enableGraphDiffMode:
      true

      ## Enable stricter URN validation logic
    strictUrnValidation: false

    ## Values specific to the unified search and browse feature.
    search_and_browse:
      show_search_v2: true
      show_browse_v2: true
      # If on, run the backfill upgrade job that generates default browse paths for relevant entities
      backfill_browse_v2: true

    ## v0.15.0+
    metadataChangeProposal:
      consumer:
        batch:
          enabled: false

    ## v0.13.4+
    mcp:
      throttle:
        # updateIntervalMs: 60000
        mceConsumer: # v0.14.2+
          enabled: false
        apiRequests: # v0.14.2+
          enabled: false
        ## Versioned MCL topic
        versioned:
          ## Whether to throttle MCP processing based on MCL backlog
          enabled: false
        #  threshold: 4000
        #  maxAttempts: 1000
        #  initialIntervalMs: 100
        #  multiplier: 10
        #  maxIntervalMs:  30000
        # Timeseries MCL topic
        timeseries:
          ## Whether to throttle MCP processing based on MCL backlog
          enabled: false
        #  threshold: 4000
        #  maxAttempts: 1000
        #  initialIntervalMs: 100
        #  multiplier: 10
        #  maxIntervalMs: 30000

    ## v0.15.0+
    entityVersioning:
      enabled: false

    ## Enables a fast path for processing events sourced from the UI, updates indices synchronously for requests originating from GraphQL
    preProcessHooksUIEnabled: true
    ## Reprocess UI events at MAE Consumer, normally if preprocess is enabled this is not required.
    reProcessUIEventHooks: false
